{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SQLContext\n",
    "import pyspark.sql.functions as sql\n",
    "import pyspark.sql.types as types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "The datasets below are on local disk and are in some scratch directories. Final datasets will be in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idigbio_full = sqlContext.read.parquet(\"../2016spr/data/idigbio/occurrence.txt.parquet\")\n",
    "bhl_full = sqlContext.read.parquet(\"../guoda-datasets/BHL/data/bhl-20160516.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined\n"
     ]
    }
   ],
   "source": [
    "# This replaces the datasets with ones that are a small subset\n",
    "#bhl = bhl_full.sample(fraction=0.01, withReplacement=False)\n",
    "#idigbio = idigbio_full.sample(fraction=0.001, withReplacement=False)\n",
    "\n",
    "bhl = sqlContext.createDataFrame(bhl_full.head(100))\n",
    "idigbio = sqlContext.createDataFrame(idigbio_full.head(10000))\n",
    "bhl.cache()\n",
    "idigbio.cache()\n",
    "print(\"defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a list of IDs\n",
    "\n",
    "From iDigBio data, make a list of unique identifiers to try to find in BHL. While there are lots of identifiers in fields, we're going to guess that the Darwin Core catalogNumber field would be the most likely to be found so only that field will be used to start. Later, other fields could be examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idigbio.registerTempTable(\"idbtable\")\n",
    "# Demonstrate using SQL syntax for working with Spark dataframes\n",
    "ids = sqlContext.sql(\"\"\"\n",
    "                       SELECT\n",
    "                        `http://rs.tdwg.org/dwc/terms/occurrenceID` as id\n",
    "                       FROM idbtable WHERE \n",
    "                        `http://rs.tdwg.org/dwc/terms/occurrenceID` != ''\n",
    "                       GROUP BY\n",
    "                        `http://rs.tdwg.org/dwc/terms/occurrenceID`\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9872\n"
     ]
    }
   ],
   "source": [
    "# This triggers running the group by and takes a few seconds\n",
    "print(ids.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for those IDs\n",
    "\n",
    "With the list of ids, go find them in BHL OCR text. Final data structure should be a list of IDs with the added column containing a list of item IDs where the iDigBio id is found and perhaps a column for the count of the items IDs in the list which is probably what we'll end up sorting on.\n",
    "\n",
    "The naive approach would be to take each document and check it for each id in turn. That would be about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "print(8000000*180000/1000000/60/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hours of work asuming we could perform 1M searches per seond. Instead, let's make a list of unique words in BHL which items those words come from. Then we can join that list to the list of catalog numbers and use aggregate functions to have our answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing BHL into words\n",
    "\n",
    "To do that, first we need to tokenize each document and for that we need a tokenizer. We'll just use spliting on whitespace as something simple.\n",
    "\n",
    "This builds a list of word & item ID combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unicode_split(s):\n",
    "    if s is not None:\n",
    "        return s.split()\n",
    "    else:\n",
    "        return []\n",
    "unicode_split_udf = sql.udf(unicode_split, types.ArrayType(types.StringType()))\n",
    "\n",
    "words = bhl.select(sql.explode(unicode_split_udf(bhl.ocrtext)).alias('word'), bhl.itemid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(word=u'Historic,', itemid=152051), Row(word=u'archived', itemid=152051), Row(word=u'document', itemid=152051), Row(word=u'Do', itemid=152051), Row(word=u'not', itemid=152051), Row(word=u'assume', itemid=152051), Row(word=u'content', itemid=152051), Row(word=u'reflects', itemid=152051), Row(word=u'current', itemid=152051), Row(word=u'scientific', itemid=152051), Row(word=u'knowledge,', itemid=152051), Row(word=u'policies,', itemid=152051), Row(word=u'or', itemid=152051), Row(word=u'practices.', itemid=152051), Row(word=u'THE', itemid=152051), Row(word=u'BEST', itemid=152051), Row(word=u'OTTHE', itemid=152051), Row(word=u'OLD,', itemid=152051), Row(word=u'NEW', itemid=152051), Row(word=u'AND', itemid=152051)]\n"
     ]
    }
   ],
   "source": [
    "print(words.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196538\n"
     ]
    }
   ],
   "source": [
    "print(words.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Summarize words\n",
    "\n",
    "`words` right now is all the words in the item but wer really only need to keep unique words since we're looking for specific terms. We can run distinct on this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(word=u'Mail', itemid=152051, count=1)\n"
     ]
    }
   ],
   "source": [
    "# could also use df.distinct() if we didn't want the counts\n",
    "words_summary = words.select(words.word, words.itemid).groupBy(words.word, words.itemid).count()\n",
    "print(words_summary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521905\n"
     ]
    }
   ],
   "source": [
    "print(words_summary.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Joining to perform the search\n",
    "\n",
    "Now we have a list of unique words in each item and a list of ids we're looking for. Let's join them and see if we have any exact matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=u'339', word=u'339', itemid=152065, count=1), Row(id=u'52390', word=u'52390', itemid=152112, count=1), Row(id=u'52390', word=u'52390', itemid=152131, count=1), Row(id=u'52390', word=u'52390', itemid=152135, count=1), Row(id=u'10363', word=u'10363', itemid=152112, count=1), Row(id=u'11920', word=u'11920', itemid=152135, count=1)]\n"
     ]
    }
   ],
   "source": [
    "joined = ids.join(words_summary, ids.id == words_summary.word, 'inner')\n",
    "print(joined.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(joined.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "This works but the output is suspicious. The few occurrenceID in the subset of items appear to all be simple numbers which we can't trust to be actual references to occurrence IDs.\n",
    "\n",
    "But, the method works. Let's repackage this as an efficient job and see if we get better results at scale. Remember we only looked at 100 BHL texts and 10k catalogNumbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
